services:
  vector_db:
    image: qdrant/qdrant:latest
    container_name: vector_db
    restart: always
    volumes:
      - ./volumes/qdrant:/qdrant/storage
    ports:
      - 6333:6333
      - 6334:6334
    expose:
      - 6333
      - 6334
      - 6335
    networks:
      - default

  xinferences:
    image: xprobe/xinference:v1.2.2
    container_name: xinference
    command: xinference-local -H 0.0.0.0
    ports:
      - "9997:9997"
    restart: always
    volumes:
      - ./volumes/.xinference:/root/.xinference
      - ./volumes/.cache/huggingface:/root/.cache/huggingface
      - ./volumes/.cache/.cache/modelscope:/root/.cache/modelscope
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["0"]
              capabilities: [gpu]


  # embedding_model:
  #   image: ghcr.io/huggingface/text-embeddings-inference:1.6
  #   container_name: embedding_model
  #   command: --model-id BAAI/bge-m3 --max-client-batch-size=1024 --max-batch-requests=1024 --max-batch-tokens=64000 --auto-truncate
  #   restart: always
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             device_ids: ["0"]
  #             capabilities: [gpu]
  #   volumes:
  #     - ./volumes/embedding_model:/data
  #   ports:
  #     - 8080:80
  #   networks:
  #     - default

  # rerank_model:
  #   image: ghcr.io/huggingface/text-embeddings-inference:1.3
  #   container_name: rerank_model
  #   command: --model-id BAAI/bge-reranker-v2-gemma --pooling mean
  #   restart: always
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             device_ids: ["0"]
  #             capabilities: [gpu]
  #   volumes:
  #     - ./volumes/rerank_model:/data
  #   ports:
  #     - 8080:80
  #   networks:
  #     - default


  # rerank_model:
  #   image: ghcr.io/huggingface/text-embeddings-inference:1.3
  #   container_name: rerank_model
  #   restart: always
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             device_ids: ["0"]
  #             capabilities: [gpu]
  #   ports:
  #     - "5002:80"
  #   volumes:
  #     - ./embedding_model_data:/data
  #   command: --model-id BAAI/bge-reranker-v2-gemma

  # llm:
  #   image: vllm/vllm-openai
  #   container_name: llms
  #   restart: always
  #   ports:
  #     - "5000:8000"
  #   volumes:
  #     - ~/.cache/huggingface:/root/.cache/huggingface
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             device_ids: ['0']
  #             capabilities: [gpu]
  #   ipc: host
  #   command: --model unsloth/Qwen2.5-7B-Instruct --max_seq_len=1000 --max_model_len=6000 --enable_prefix_caching # --api-key=empty
  #   networks:
  #     - default



